{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labs 5 onwards: TensorFlow with Financial Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warning\n",
    "The following discussion and notebook is a well-known example demonstrating how tensorflow could be useful for financial data.\n",
    "However, the story below, and the code, have a serious flaw. \n",
    "Can you spot it?\n",
    "\n",
    "Despite the flaw, it is worth trying to understand how the code works and what is the undelying idea.\n",
    "\n",
    "\n",
    "## The Story begins here\n",
    "\n",
    "\n",
    "Today, we have more data at our disposal than ever, more sources of data, and more frequent delivery of that data. New sources include new exchanges, social media outlets, and news sources. Naturally, more and different analysis techniques are being brought to bear as a result. Most of the modern analysis techniques aren't different in the sense of being new, and they all have their basis in statistics, but their applicability has closely followed the amount of computing power available. The growth in available computing power is faster than the growth in time series volumes, so it is possible to analyze time series today at scale in ways that weren't previously practical.\n",
    "\n",
    "In particular, machine learning techniques, especially deep learning, hold promise for time series analysis. As time series become more dense and many time series overlap, machine learning offers a way to separate the signal from the noise, even when the noise can seem overwhelming. Deep learning holds potential because it is often a good fit for the seemingly random nature of financial time series.\n",
    "\n",
    "In this notebook, we will first obtain data for a number of financial markets. We will then transform the data into a usable format and perform exploratory data analysis in order to explore and validate a hypothesis. Then, we will use Tensorflow to build, train and evaluate a number of models for predicting what will happen in financial markets\n",
    "\n",
    "<h3>The working hypothesis</h3>\n",
    "Our thinking is as follows: financial markets are increasingly global, and if we follow the sun from Asia to Europe to the US and so on, we can use information from an earlier time zone to our advantage in a later time zone.\n",
    "The following table shows a number of stock market indices from around the globe, their closing times in Eastern Standard Time (EST), and the delay in hours between the close that index and the close of the S&P 500 in New York. This makes EST the base time zone. For example, Australian markets close for the day 15 hours before US markets close. If the close of the All Ords in Australia is a useful predictor of the close of the S&P 500 for a given day we can use that information to guide our trading activity. Continuing our example of the Australian All Ords, if this index closes up and we think that means the S&P 500 will close up as well then we should either buy stocks that compose the S&P 500 or, more likely, an ETF that tracks the S&P 500. In reality, the situation is more complex because there are commissions and tax to account for. But as a first approximation, we'll assume an index closing up indicates a gain, and vice-versa.\n",
    "\n",
    "<table>\n",
    "<tr><th>Index</th><th>Country</th>\t<th>Closing Time (EST)</th>\t<th>Hours Before S&P Close</th></tr>\n",
    "<tr><td>All Ords</td><td>Australia</td>\t<td>0100</td>\t<td>15</td></tr>\n",
    "<tr><td>Nikkei 225</td><td>Japan</td>\t<td>0200</td>\t<td>14</td></tr>\n",
    "<tr><td>Hang Seng</td><td>Hong Kong</td>\t<td>0400</td>\t<td>12</td></tr>\n",
    "<tr><td>DAX</td><td>Germany</td>\t<td>1130</td>\t<td>4.5</td></tr>\n",
    "<tr><td>FTSE 100</td><td>UK</td>\t<td>1130</td>\t<td>4.5</td></tr>\n",
    "<tr><td>NYSE Composite</td><td>US</td>\t<td>1600</td>\t<td>0</td></tr>\n",
    "<tr><td>Dow Jones Industrial Average</td><td>US</td>\t<td>1600</td>\t<td>0</td></tr>\n",
    "<tr><td>S&P 500</td><td>US</td>\t<td>1600</td>\t<td>0</td></tr>\n",
    "</table>\n",
    "\n",
    "<h3>Set up</h3>\n",
    "First, we will import some necessary libraries. We have already discussed numpy, pandas, tensorflow, and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sofia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Get the data</h3>\n",
    "The data covers the new millenium (2000 onwards) and is available in the data directory. Data comes from the S&P 500 (S&P), NYSE, Dow Jones Industrial Average (DJIA), Nikkei 225 (Nikkei), Hang Seng, FTSE 100 (FTSE), DAX, and All Ordinaries (AORD) indices. \n",
    "\n",
    "We access this data as Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/2k/YAHOO-INDEX_GSPC.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Reading the datasets in different dataframes using Pandas\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m snp \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/2k/YAHOO-INDEX_GSPC.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m nyse \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/2k/YAHOO-INDEX_NYA.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m      5\u001b[0m djia \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/2k/YAHOO-INDEX_DJI.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m) \n",
      "File \u001b[1;32mc:\\Users\\sofia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sofia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\sofia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sofia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\sofia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/2k/YAHOO-INDEX_GSPC.csv'"
     ]
    }
   ],
   "source": [
    "#Reading the datasets in different dataframes using Pandas\n",
    "\n",
    "snp = pd.read_csv(\"data/2k/YAHOO-INDEX_GSPC.csv\", index_col='Date')\n",
    "nyse = pd.read_csv(\"data/2k/YAHOO-INDEX_NYA.csv\", index_col='Date') \n",
    "djia = pd.read_csv(\"data/2k/YAHOO-INDEX_DJI.csv\", index_col='Date') \n",
    "nikkei = pd.read_csv(\"data/2k/YAHOO-INDEX_N225.csv\", index_col='Date') \n",
    "hangseng = pd.read_csv(\"data/2k/YAHOO-INDEX_HSI.csv\", index_col='Date') \n",
    "ftse = pd.read_csv(\"data/2k/FTSE100.csv\", index_col='Date') \n",
    "dax = pd.read_csv(\"data/2k/YAHOO-INDEX_GDAXI.csv\", index_col='Date') \n",
    "aord = pd.read_csv(\"data/2k/YAHOO-INDEX_AORD.csv\", index_col='Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Munge the data</h3>\n",
    "In the first instance, munging the data is straightforward. The closing prices are of interest, so for convenience extract the closing prices for each of the indices into a single Pandas DataFrame, called <i>closing_data</i>. Because not all of the indices have the same number of values, mainly due to bank holidays, we'll fill the gaps. This means that, if a value isn't available for day $N$, fill it with the value for another day, such as $N+1$ or $N+2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'snp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m closing_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m----> 3\u001b[0m closing_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnp_close\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msnp\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m closing_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnyse_close\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m nyse[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m closing_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdjia_close\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m djia[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClose\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'snp' is not defined"
     ]
    }
   ],
   "source": [
    "closing_data = pd.DataFrame()\n",
    "\n",
    "closing_data['snp_close'] = snp['Close']\n",
    "closing_data['nyse_close'] = nyse['Close']\n",
    "closing_data['djia_close'] = djia['Close']\n",
    "closing_data['nikkei_close'] = nikkei['Close']\n",
    "closing_data['hangseng_close'] = hangseng['Close']\n",
    "closing_data['ftse_close'] = ftse['Close']\n",
    "closing_data['dax_close'] = dax['Close']\n",
    "closing_data['aord_close'] = aord['Close']\n",
    "\n",
    "# Pandas includes a very convenient function for filling gaps in the data.\n",
    "closing_data = closing_data.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have sourced some years of time series for eight financial indices, combined the data into a single data structure, and harmonized the data to have the same number of entries, by using only the 20 lines of code in this notebook. \n",
    "\n",
    "<h3>Exploratory data analysis</h3>\n",
    "Exploratory Data Analysis (EDA) is foundational to working with machine learning, and any other sort of analysis. EDA means getting to know our data, so that when we build models, we build them based on an actual, physical understanding of the data, and not on assumptions. We can still make assumptions of course, but EDA means we will understand our assumptions and why we are making those assumptions. \n",
    "\n",
    "First, take a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closing_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the various indices operate on scales differing by orders of magnitude. It is best to scale the data so that, for example, operations involving multiple indices aren't unduly influenced by a single, massive index.\n",
    "\n",
    "Let's first plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_frame = pd.DataFrame()\n",
    "\n",
    "plot_frame = pd.concat([closing_data['snp_close'],\n",
    "  closing_data['nyse_close'],\n",
    "  closing_data['djia_close'],\n",
    "  closing_data['nikkei_close'],\n",
    "  closing_data['hangseng_close'],\n",
    "  closing_data['ftse_close'],\n",
    "  closing_data['dax_close'],\n",
    "  closing_data['aord_close']], axis=1)\n",
    "plot_frame.plot(figsize=(20, 15), grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the structure isn't entirely visible for all indices. Notice that there is an index ranging below $5\\, 000$ while other indices range from $20\\, 000$ to $30\\, 000$. So, let's divide each value in an individual index by the maximum value for that index, and then replot. The maximum value of all indices will then be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closing_data['snp_close_scaled'] = closing_data['snp_close'] / max(closing_data['snp_close'])\n",
    "closing_data['nyse_close_scaled'] = closing_data['nyse_close'] / max(closing_data['nyse_close'])\n",
    "closing_data['djia_close_scaled'] = closing_data['djia_close'] / max(closing_data['djia_close'])\n",
    "closing_data['nikkei_close_scaled'] = closing_data['nikkei_close'] / max(closing_data['nikkei_close'])\n",
    "closing_data['hangseng_close_scaled'] = closing_data['hangseng_close'] / max(closing_data['hangseng_close'])\n",
    "closing_data['ftse_close_scaled'] = closing_data['ftse_close'] / max(closing_data['ftse_close'])\n",
    "closing_data['dax_close_scaled'] = closing_data['dax_close'] / max(closing_data['dax_close'])\n",
    "closing_data['aord_close_scaled'] = closing_data['aord_close'] / max(closing_data['aord_close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = pd.concat([closing_data['snp_close_scaled'],\n",
    "  closing_data['nyse_close_scaled'],\n",
    "  closing_data['djia_close_scaled'],\n",
    "  closing_data['nikkei_close_scaled'],\n",
    "  closing_data['hangseng_close_scaled'],\n",
    "  closing_data['ftse_close_scaled'],\n",
    "  closing_data['dax_close_scaled'],\n",
    "  closing_data['aord_close_scaled']], axis=1).plot(figsize=(20, 15), grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, over the period of interest, these indices are correlated. Notice that sudden drops from economic events happened globally to all indices, and they otherwise exhibited general rises. This is a good start, though not the complete story. \n",
    "\n",
    "Now, we plot [autocorrelations](https://en.wikipedia.org/wiki/Autocorrelation) for each of the indices. The autocorrelations determine correlations between current values of the index and lagged values of the same index. The goal is to determine whether the lagged values are reliable indicators of the current values. If they are, then we've identified a correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(15)\n",
    "\n",
    "_ = autocorrelation_plot(closing_data['snp_close'], label='snp_close')\n",
    "_ = autocorrelation_plot(closing_data['nyse_close'], label='nyse_close')\n",
    "_ = autocorrelation_plot(closing_data['djia_close'], label='djia_close')\n",
    "_ = autocorrelation_plot(closing_data['nikkei_close'], label='nikkei_close')\n",
    "_ = autocorrelation_plot(closing_data['hangseng_close'], label='hangseng_close')\n",
    "_ = autocorrelation_plot(closing_data['ftse_close'], label='ftse_close')\n",
    "_ = autocorrelation_plot(closing_data['dax_close'], label='dax_close')\n",
    "_ = autocorrelation_plot(closing_data['aord_close'], label='aord_close')\n",
    "\n",
    "_ = plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some strong autocorrelations, both positive and negative for several days. This tells us something we perhaps intuitively expect: if an index is rising it tends to carry on rising, and vice-versa. It should be encouraging that what we see here conforms to what we know about financial markets.\n",
    "\n",
    "Next, look at a scatter matrix, showing everything plotted against everything, to see how indices are correlated with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_frame=pd.concat([closing_data['snp_close_scaled'],\n",
    "  closing_data['nyse_close_scaled'],\n",
    "  closing_data['djia_close_scaled'],\n",
    "  closing_data['nikkei_close_scaled'],\n",
    "  closing_data['hangseng_close_scaled'],\n",
    "  closing_data['ftse_close_scaled'],\n",
    "  closing_data['dax_close_scaled'],\n",
    "  closing_data['aord_close_scaled']], axis=1)\n",
    "_ = scatter_matrix(plot_frame, figsize=(20, 20), diagonal='kde')\n",
    "#plot_frame.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see significant correlations across the board, further evidence that the working hypothesis is sound and one market can be influenced by another. \n",
    "\n",
    "The actual value of an index is not that useful for modeling. It can be a useful indicator, but to get to the heart of the matter, we need a time series that is stationary in the mean, thus having no trend in the data. There are various ways of doing that, but they all essentially look at the difference between values, rather than the absolute value. In the case of market data, the usual practice is to work with logged returns, calculated as the natural logarithm of the index today divided by the index yesterday:\n",
    "\n",
    "$$ln\\left(\\frac{V_t}{V_{t-1}}\\right)$$\n",
    "\n",
    "There are more reasons why the log return is preferable to the percent return (for example the log is normally distributed and additive), but they don't matter much for this work. What matters is to get to a stationary time series. Let's calculate and plot the log returns in a new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_return_data = pd.DataFrame()\n",
    "\n",
    "log_return_data['snp_log_return'] = np.log(closing_data['snp_close']/closing_data['snp_close'].shift())\n",
    "log_return_data['nyse_log_return'] = np.log(closing_data['nyse_close']/closing_data['nyse_close'].shift())\n",
    "log_return_data['djia_log_return'] = np.log(closing_data['djia_close']/closing_data['djia_close'].shift())\n",
    "log_return_data['nikkei_log_return'] = np.log(closing_data['nikkei_close']/closing_data['nikkei_close'].shift())\n",
    "log_return_data['hangseng_log_return'] = np.log(closing_data['hangseng_close']/closing_data['hangseng_close'].shift())\n",
    "log_return_data['ftse_log_return'] = np.log(closing_data['ftse_close']/closing_data['ftse_close'].shift())\n",
    "log_return_data['dax_log_return'] = np.log(closing_data['dax_close']/closing_data['dax_close'].shift())\n",
    "log_return_data['aord_log_return'] = np.log(closing_data['aord_close']/closing_data['aord_close'].shift())\n",
    "\n",
    "log_return_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the log returns, you should see that the mean, min, max are all similar. You could go further and center the series on zero, scale them, and normalize the standard deviation, but there's no need to do that at this point. Let's move forward with plotting the data, and iterate if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pd.concat([\n",
    "  log_return_data['snp_log_return'],\n",
    "  log_return_data['nyse_log_return'],\n",
    "  log_return_data['djia_log_return'],\n",
    "  log_return_data['nikkei_log_return'],\n",
    "  log_return_data['hangseng_log_return'],\n",
    "  log_return_data['ftse_log_return'],\n",
    "  log_return_data['dax_log_return'],\n",
    "  log_return_data['aord_log_return']], axis=1).plot(figsize=(20, 15), grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the plot that the log returns of our indices are similarly scaled and centered, with no visible trend in the data. It's looking good, so now look at autocorrelations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(15)\n",
    "\n",
    "_ = autocorrelation_plot(log_return_data['snp_log_return'], label='snp_log_return')\n",
    "_ = autocorrelation_plot(log_return_data['nyse_log_return'], label='nyse_log_return')\n",
    "_ = autocorrelation_plot(log_return_data['djia_log_return'], label='djia_log_return')\n",
    "_ = autocorrelation_plot(log_return_data['nikkei_log_return'], label='nikkei_log_return')\n",
    "_ = autocorrelation_plot(log_return_data['hangseng_log_return'], label='hangseng_log_return')\n",
    "_ = autocorrelation_plot(log_return_data['ftse_log_return'], label='ftse_log_return')\n",
    "_ = autocorrelation_plot(log_return_data['dax_log_return'], label='dax_log_return')\n",
    "_ = autocorrelation_plot(log_return_data['aord_log_return'], label='aord_log_return')\n",
    "\n",
    "_ = plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No autocorrelations are visible in the plot. Individual financial markets are Markov processes and knowledge of history doesn't allow us to predict the future. We now have time series for the indices, stationary in the mean, similarly centered and scaled. \n",
    "\n",
    "Now start to look for signals to try to predict the close of the S&P 500. Let's look at a scatterplot to see how the log return indices correlate with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = scatter_matrix(log_return_data, figsize=(20, 20), diagonal='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The story with the previous scatter plot for log returns is more subtle and more interesting. The US indices are strongly correlated, as expected. The other indices, less so, which is also expected. But there is structure and signal there. Now let's move forward and start to quantify it so we can start to choose features for our model.\n",
    "First look at how the log returns for the closing value of the S&P 500 correlate with the closing values of other indices available on the same day. This essentially means to assume the indices that close before the S&P 500 (non-US indices) are available and the others (US indices) are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame()\n",
    "tmp['snp_0'] = log_return_data['snp_log_return']\n",
    "tmp['nyse_1'] = log_return_data['nyse_log_return'].shift()\n",
    "tmp['djia_1'] = log_return_data['djia_log_return'].shift()\n",
    "tmp['ftse_0'] = log_return_data['ftse_log_return']\n",
    "tmp['dax_0'] = log_return_data['dax_log_return']\n",
    "tmp['hangseng_0'] = log_return_data['hangseng_log_return']\n",
    "tmp['nikkei_0'] = log_return_data['nikkei_log_return']\n",
    "tmp['aord_0'] = log_return_data['aord_log_return']\n",
    "tmp.corr().iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are directly working based on the working hypothesis. We're correlating the close of the S&P 500 with signals available before the close of the S&P 500. We see that the S&P 500 close is correlated with European indices at around 0.56 for FTSE, 0.61 for DAX, which is a strong correlation, and Asian/Oceanian indices at around 0.17-0.22, which is a significant correlation, but not with US indices. We have available signals from other indices and regions for our model.\n",
    "\n",
    "Now look at how the log returns for the S&P closing values correlate with index values from the previous day to see if the previous closing is predictive. Following from the hypothesis that financial markets are Markov processes, there should be little or no value in historical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame()\n",
    "tmp['snp_0'] = log_return_data['snp_log_return']\n",
    "tmp['nyse_1'] = log_return_data['nyse_log_return'].shift(2)\n",
    "tmp['djia_1'] = log_return_data['djia_log_return'].shift(2)\n",
    "tmp['ftse_0'] = log_return_data['ftse_log_return'].shift()\n",
    "tmp['dax_0'] = log_return_data['dax_log_return'].shift()\n",
    "tmp['hangseng_0'] = log_return_data['hangseng_log_return'].shift()\n",
    "tmp['nikkei_0'] = log_return_data['nikkei_log_return'].shift()\n",
    "tmp['aord_0'] = log_return_data['aord_log_return'].shift()\n",
    "tmp.corr().iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see little to no correlation in this data, meaning that yesterday's values are no practical help in predicting today's close. Let's go one step further and look at correlations between today and the the day before yesterday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame()\n",
    "tmp['snp_0'] = log_return_data['snp_log_return']\n",
    "tmp['nyse_1'] = log_return_data['nyse_log_return'].shift(3)\n",
    "tmp['djia_1'] = log_return_data['djia_log_return'].shift(3)\n",
    "tmp['ftse_0'] = log_return_data['ftse_log_return'].shift(2)\n",
    "tmp['dax_0'] = log_return_data['dax_log_return'].shift(2)\n",
    "tmp['hangseng_0'] = log_return_data['hangseng_log_return'].shift(2)\n",
    "tmp['nikkei_0'] = log_return_data['nikkei_log_return'].shift(2)\n",
    "tmp['aord_0'] = log_return_data['aord_log_return'].shift(2)\n",
    "\n",
    "tmp.corr().iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, there are little to no correlations.\n",
    "<h3>Summing up the EDA</h3>\n",
    "At this point, we've done a good enough job of exploratory data analysis. We have visualized the data and come to know it better. We have transformed it into a form that is useful for modelling, log returns, and looked at how indices relate to each other. We have seen that indices from Europe strongly correlate with US indices, and that indices from Asia/Oceania significantly correlate with those same indices for a given day. We've also seen that if we look at historical values, they do not correlate with today's values. Summing up:\n",
    "<ul>\n",
    "<li>European indices from the same day were a strong predictor for the S&P 500 close.\n",
    "<li>Asian/Oceanian indices from the same day were a significant predictor for the S&P 500 close.\n",
    "<li>Indices from previous days were not good predictors for the S&P close.\n",
    "</ul>\n",
    "\n",
    "<h3>Feature selection</h3>\n",
    "At this point, we can see a model:\n",
    "<ul>\n",
    "<li>We'll predict whether the S&P 500 close today will be higher or lower than yesterday.\n",
    "<li>We'll use all our data sources: NYSE, DJIA, Nikkei, Hang Seng, FTSE, DAX, AORD.\n",
    "<li>We'll use three sets of data points — $T$, $T-1$, and $T-2$ — where we take the data available on day $T$ or $T-n$, meaning today's non-US data and yesterday's US data.\n",
    "</ul>\n",
    "\n",
    "Predicting whether the log return of the S&P 500 is positive or negative is a classification problem. That is, we want to choose one option from a finite set of options, in this case positive or negative. This is the base case of classification where we have only two values to choose from, known as binary classification, or logistic regression.\n",
    "\n",
    "This uses the findings from of our exploratory data analysis, namely that log returns from other regions on a given day are strongly correlated with the log return of the S&P 500, and there are stronger correlations from those regions that are geographically closer with respect to time zones. However, our models also use data outside of those findings. For example, we use data from the past few days in addition to today. There are two reasons for using this additional data. First, we're adding additional features to our model for the purpose of this solution to see how things perform (which is of course not a good reason for adding features outside of a tutorial setting). Second, machine learning models are very good at finding weak signals from data.\n",
    "\n",
    "In machine learning, as in most things, there are subtle tradeoffs happening, but in general good data is better than good algorithms, which are better than good frameworks. We need all three pillars but in that order of importance: data, algorithms, frameworks.\n",
    "\n",
    "<h3>TensorFlow</h3>\n",
    "\n",
    "We discussed TensorFlow in the previous lab session. It is an open source software library, initiated by Google, for numerical computation using data flow graphs. It is a great framework for machine learning because it is expressive, efficient, and easy to use.\n",
    "<h4>Feature engineering for TensorFlow</h4>\n",
    "From a training and testing perspective, time series data is easy. Training data should come from events that happened before test data events, and be contiguous in time. Otherwise, our model would be trained on events from \"the future\", at least as compared to the test data. It would then likely perform badly in practice, because we can’t really have access to data from the future. That means random sampling or cross validation don't apply to time series data. Let's decide on a training-versus-testing split, and divide the data into training and test datasets.\n",
    "In this case, we'll create the features together with two additional columns:\n",
    "<ul>\n",
    "<li>snp_log_return_positive, which is 1 if the log return of the S&P 500 close is positive, and 0 otherwise.\n",
    "<li>snp_log_return_negative, which is 1 if the log return of the S&P 500 close is negative, and 0 otherwise.\n",
    "</ul>\n",
    "\n",
    "Now, logically we could encode this information in one column, named snp_log_return, which is 1 if positive and 0 if negative, but we'll prefer a different approach that allows for many different potential values to choose from, and a form or encoding for these options called one-hot encoding. One-hot encoding means that each choice is an entry in an array, and the actual value has an entry of 1 with all other values being 0. This encoding (i.e. a single 1 in an array of 0s) is for the input of the model, where you categorically know which value is correct. A variation of this is used for the output, where each entry in the array contains the probability of the answer being that choice. We can then choose the most likely value by choosing the highest probability, together with having a measure of the confidence we can place in that answer relative to other answers.\n",
    "\n",
    "We'll use 80% of our data for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_return_data['snp_log_return_positive'] = 0\n",
    "log_return_data.loc[log_return_data['snp_log_return'] >= 0, 'snp_log_return_positive'] = 1\n",
    "log_return_data['snp_log_return_negative'] = 0\n",
    "log_return_data.loc[log_return_data['snp_log_return'] < 0, 'snp_log_return_negative'] = 1\n",
    "\n",
    "training_test_data = pd.DataFrame(\n",
    "  columns=[\n",
    "    'snp_log_return_positive', 'snp_log_return_negative',\n",
    "    'snp_log_return_1', 'snp_log_return_2', 'snp_log_return_3',\n",
    "    'nyse_log_return_1', 'nyse_log_return_2', 'nyse_log_return_3',\n",
    "    'djia_log_return_1', 'djia_log_return_2', 'djia_log_return_3',\n",
    "    'nikkei_log_return_0', 'nikkei_log_return_1', 'nikkei_log_return_2',\n",
    "    'hangseng_log_return_0', 'hangseng_log_return_1', 'hangseng_log_return_2',\n",
    "    'ftse_log_return_0', 'ftse_log_return_1', 'ftse_log_return_2',\n",
    "    'dax_log_return_0', 'dax_log_return_1', 'dax_log_return_2',\n",
    "    'aord_log_return_0', 'aord_log_return_1', 'aord_log_return_2'])\n",
    "\n",
    "for i in range(7, len(log_return_data)):\n",
    "  snp_log_return_positive = log_return_data['snp_log_return_positive'].iloc[i]\n",
    "  snp_log_return_negative = log_return_data['snp_log_return_negative'].iloc[i]\n",
    "  snp_log_return_1 = log_return_data['snp_log_return'].iloc[i-1]\n",
    "  snp_log_return_2 = log_return_data['snp_log_return'].iloc[i-2]\n",
    "  snp_log_return_3 = log_return_data['snp_log_return'].iloc[i-3]\n",
    "  nyse_log_return_1 = log_return_data['nyse_log_return'].iloc[i-1]\n",
    "  nyse_log_return_2 = log_return_data['nyse_log_return'].iloc[i-2]\n",
    "  nyse_log_return_3 = log_return_data['nyse_log_return'].iloc[i-3]\n",
    "  djia_log_return_1 = log_return_data['djia_log_return'].iloc[i-1]\n",
    "  djia_log_return_2 = log_return_data['djia_log_return'].iloc[i-2]\n",
    "  djia_log_return_3 = log_return_data['djia_log_return'].iloc[i-3]\n",
    "  nikkei_log_return_0 = log_return_data['nikkei_log_return'].iloc[i]\n",
    "  nikkei_log_return_1 = log_return_data['nikkei_log_return'].iloc[i-1]\n",
    "  nikkei_log_return_2 = log_return_data['nikkei_log_return'].iloc[i-2]\n",
    "  hangseng_log_return_0 = log_return_data['hangseng_log_return'].iloc[i]\n",
    "  hangseng_log_return_1 = log_return_data['hangseng_log_return'].iloc[i-1]\n",
    "  hangseng_log_return_2 = log_return_data['hangseng_log_return'].iloc[i-2]\n",
    "  ftse_log_return_0 = log_return_data['ftse_log_return'].iloc[i]\n",
    "  ftse_log_return_1 = log_return_data['ftse_log_return'].iloc[i-1]\n",
    "  ftse_log_return_2 = log_return_data['ftse_log_return'].iloc[i-2]\n",
    "  dax_log_return_0 = log_return_data['dax_log_return'].iloc[i]\n",
    "  dax_log_return_1 = log_return_data['dax_log_return'].iloc[i-1]\n",
    "  dax_log_return_2 = log_return_data['dax_log_return'].iloc[i-2]\n",
    "  aord_log_return_0 = log_return_data['aord_log_return'].iloc[i]\n",
    "  aord_log_return_1 = log_return_data['aord_log_return'].iloc[i-1]\n",
    "  aord_log_return_2 = log_return_data['aord_log_return'].iloc[i-2]\n",
    "    \n",
    "    \n",
    "  df_dictionary = pd.DataFrame({'snp_log_return_positive':snp_log_return_positive,\n",
    "    'snp_log_return_negative':snp_log_return_negative,\n",
    "    'snp_log_return_1':snp_log_return_1,\n",
    "    'snp_log_return_2':snp_log_return_2,\n",
    "    'snp_log_return_3':snp_log_return_3,\n",
    "    'nyse_log_return_1':nyse_log_return_1,\n",
    "    'nyse_log_return_2':nyse_log_return_2,\n",
    "    'nyse_log_return_3':nyse_log_return_3,\n",
    "    'djia_log_return_1':djia_log_return_1,\n",
    "    'djia_log_return_2':djia_log_return_2,\n",
    "    'djia_log_return_3':djia_log_return_3,\n",
    "    'nikkei_log_return_0':nikkei_log_return_0,\n",
    "    'nikkei_log_return_1':nikkei_log_return_1,\n",
    "    'nikkei_log_return_2':nikkei_log_return_2,\n",
    "    'hangseng_log_return_0':hangseng_log_return_0,\n",
    "    'hangseng_log_return_1':hangseng_log_return_1,\n",
    "    'hangseng_log_return_2':hangseng_log_return_2,\n",
    "    'ftse_log_return_0':ftse_log_return_0,\n",
    "    'ftse_log_return_1':ftse_log_return_1,\n",
    "    'ftse_log_return_2':ftse_log_return_2,\n",
    "    'dax_log_return_0':dax_log_return_0,\n",
    "    'dax_log_return_1':dax_log_return_1,\n",
    "    'dax_log_return_2':dax_log_return_2,\n",
    "    'aord_log_return_0':aord_log_return_0,\n",
    "    'aord_log_return_1':aord_log_return_1,\n",
    "    'aord_log_return_2':aord_log_return_2}, index = [0])\n",
    "  \n",
    "  training_test_data = pd.concat([training_test_data, df_dictionary], ignore_index = True) \n",
    "training_test_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors_tf = training_test_data[training_test_data.columns[2:]]\n",
    "\n",
    "classes_tf = training_test_data[training_test_data.columns[:2]]\n",
    "\n",
    "training_set_size = int(len(training_test_data) * 0.8)\n",
    "test_set_size = len(training_test_data) - training_set_size\n",
    "\n",
    "training_predictors = predictors_tf[:training_set_size]\n",
    "training_classes = classes_tf[:training_set_size]\n",
    "test_predictors = predictors_tf[training_set_size:]\n",
    "test_classes = classes_tf[training_set_size:]\n",
    "\n",
    "training_predictors.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictors.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some metrics here to evaluate the models.\n",
    "<ul>\n",
    "<li>Precision - The ability of the classifier not to label as positive a sample that is negative.\n",
    "<li>Recall - The ability of the classifier to find all the positive samples.\n",
    "<li>F1 Score - A weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
    "<li>Accuracy - The percentage correctly predicted in the test data.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_confusion_metrics(predicted_classes, actual_classes):\n",
    "  predictions = tf.argmax(predicted_classes, 1)\n",
    "  actuals = tf.argmax(actual_classes, 1)\n",
    "\n",
    "  ones_like_actuals = tf.ones_like(actuals)\n",
    "  zeros_like_actuals = tf.zeros_like(actuals)\n",
    "  ones_like_predictions = tf.ones_like(predictions)\n",
    "  zeros_like_predictions = tf.zeros_like(predictions)\n",
    "\n",
    "  #true positive  \n",
    "  tp_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, ones_like_actuals), \n",
    "        tf.equal(predictions, ones_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  #true negative  \n",
    "  tn_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, zeros_like_actuals), \n",
    "        tf.equal(predictions, zeros_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  #false positive  \n",
    "  fp_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, zeros_like_actuals), \n",
    "        tf.equal(predictions, ones_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  #false negative  \n",
    "  fn_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, ones_like_actuals), \n",
    "        tf.equal(predictions, zeros_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  tpr = float(tp_op)/(float(tp_op) + float(fn_op))\n",
    "  fpr = float(fp_op)/(float(fp_op) + float(tn_op))\n",
    "\n",
    "  accuracy = (float(tp_op) + float(tn_op))/(float(tp_op) + float(fp_op) + float(fn_op) + float(tn_op))\n",
    "\n",
    "  recall = tpr\n",
    "  precision = float(tp_op)/(float(tp_op) + float(fp_op))\n",
    "  \n",
    "  f1_score = (2 * (precision * recall)) / (precision + recall)\n",
    "  \n",
    "  print('Precision = ', precision)\n",
    "  print('Recall = ', recall)\n",
    "  print('F1 Score = ', f1_score)\n",
    "  print('Accuracy = ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Binary classification with TensorFlow</h3>\n",
    "\n",
    "Now, get some action on the tensors. The model is binary classification expressed in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables for the number of predictors and number of classes \n",
    "num_predictors = len(training_predictors.columns) # 24 in the default case\n",
    "num_classes = len(training_classes.columns) # 2 in the default case\n",
    "\n",
    "# Define a matrix of weights and initialize it with some small random values.\n",
    "weights = tf.Variable(tf.cast(tf.random.truncated_normal([num_predictors, num_classes], stddev=0.0001), tf.double))\n",
    "biases = tf.Variable(tf.cast(tf.ones([num_classes]), tf.double))\n",
    "\n",
    "# Define our model\n",
    "# Here we take a softmax regression of the product of our feature data and weights\n",
    "def bin_classify(train_data):\n",
    "    return tf.nn.softmax(tf.matmul(train_data.values, weights) + biases)\n",
    "\n",
    "# Define a cost function (we're using the cross entropy)\n",
    "def cost(labeled_data, train_data):\n",
    "    loss = -tf.reduce_sum(labeled_data.values.reshape(len(training_classes), 2)*tf.math.log(train_data))\n",
    "    return loss\n",
    "\n",
    "# Here we use gradient descent with a learning rate of 0.01 using the cost function we just defined\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train our model in the following snippet. We'll train the model over 10,000 iterations using the full training dataset each time. Every 2000 iterations, we'll assess the accuracy of the model on the training data to assess progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization process. This extends the linear regression example seen in the previous lab session.\n",
    "def run_optimization():\n",
    "    # Wrap computation inside a GradientTape for automatic differentiation\n",
    "    with tf.GradientTape() as g:\n",
    "        predicted_classes = bin_classify(training_predictors)\n",
    "        loss = cost(training_classes, predicted_classes)\n",
    "    \n",
    "    vars = [weights, biases]\n",
    "    # Compute gradients. Since we have two variables, we get two gradients\n",
    "    gradients = g.gradient(loss, vars)\n",
    "    \n",
    "    # Update weights and biases following gradients. What does zip do? Can you figure it out?\n",
    "    optimizer.apply_gradients(zip(gradients, vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = training_classes.values.reshape(len(training_classes.values), 2)\n",
    "\n",
    "for i in range(1, 10001):\n",
    "    run_optimization()\n",
    "    if i%2000 == 0:\n",
    "        predicted = bin_classify(training_predictors)\n",
    "        correct_prediction = tf.equal(tf.argmax(predicted, 1), tf.argmax(tf.convert_to_tensor(actual, dtype = tf.float32), 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print('Iteration:', i, \"Accuracy:\", accuracy.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy of 72% on the training data is certainly fine and far better than random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to apply the weights and the biases on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what we predict for the test data\n",
    "predicted_test = bin_classify(test_predictors)\n",
    "actual_test = test_classes.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to obtain some metrics, see above for the definition of tf_confusion_metrics\n",
    "tf_confusion_metrics(predicted_test, actual_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Binary Classification with Keras and a feed-forward neural network </h3>\n",
    "\n",
    "Let's now follow a different approach and build a proper feed-forward neural net with two hidden layers. Again, we'll train the model over a number of epochs using the full training dataset each time. \n",
    "\n",
    "We first define the layers. The first two have 20 nodes each and use the ReLU activation function. The output layer uses a softmax activation function. Don't worry if these notions seem entirely new; they probably are. We will cover them in the next lectures in more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(20, activation='relu'),\n",
    "    tf.keras.layers.Dense(20, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now get rid of the one-hot encoding user earlier.\n",
    "# as the label data consist of two columns, we simply select the second column as the correct label.\n",
    "# indeed, a label of [0] [1] in one-hot encoding implies that the input belongs in the second category.\n",
    "# Then, it belongs in category 1 (important: category 0 is the first). Similarly, a label of [1][0] implies\n",
    "# that the input belongs in the first category. In any case, the second value in the one-hot encoding provides\n",
    "# the correct category. Important: This property does not hold in general, but only since we have two classes\n",
    "\n",
    "modified_classes = training_classes[\"snp_log_return_negative\"]\n",
    "modified_test_classes = test_classes[\"snp_log_return_negative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(training_predictors.values, modified_classes.values, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_predictors,  modified_test_classes, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>What is the flaw?</h3>\n",
    "Feel free to ask me about it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
